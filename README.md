# stereowipe
Data and code for stereowipe, a comprehensive stereotyping evaluation benchmark for LLMs

## Overview

StereoWipe is a benchmark designed to evaluate Large Language Models (LLMs) for stereotyping. It provides datasets, metrics, and tools to assess the extent and nature of stereotypical associations present in model responses. It utilizes an LLM-as-a-Judge paradigm, where configurable AI judges assess model outputs.

## Project Structure (Key Components)

- `biaswipe/`: Core Python package for the benchmark.
    - `cli.py`: Command-line interface for running benchmarks.
    - `data_loader.py`: Utilities for loading prompts, annotations, and model responses.
    - `judge.py`: Implements the LLM-as-a-Judge abstraction, including specific judge classes (e.g., OpenAI, Anthropic, Mock) and caching.
    - `metrics.py`: Implementation of stereotyping metrics (SR, SSS, CSSS, WOSI).
    - `scoring.py`: Main logic for scoring model responses, now using the LLM-as-a-Judge system.
    - `report.py`: Utility for generating JSON reports from benchmark results.
- `biaswipe_viewer/`: Flask web application for viewing reports.
    - `webserver.py`: The Flask web server.
    - `templates/`: HTML templates for the web viewer.
- `sample_data/`: Directory containing sample JSON files for prompts, annotations, model responses, and category weights.
- `tests/`: Unit tests for the project.
- `judge_prompt.txt`: The master prompt template used by the AI judges. Located in the project root.
- `json_output_schema.json`: Defines the expected JSON structure for a judge's response. Located in the project root.
- `.cache/judgments/`: Default directory where individual judge responses are cached to speed up re-runs.
- `requirements.txt`: Python package dependencies.
- `report.json`: Default output file for benchmark reports generated by the CLI.

## Getting Started

### Installation

1.  Clone the repository:
    ```bash
    git clone <repository_url>
    cd stereowipe
    ```
2.  Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### Configuration

#### 1. Judge Prompt (`judge_prompt.txt`)
The `judge_prompt.txt` file in the project root contains the template prompt that all AI judges will use. You can inspect or modify this file to change the instructions given to the judges. The default prompt guides the AI to identify stereotypes and assess their severity.

#### 2. API Keys for Judges
If you plan to use judges that rely on external APIs (e.g., "openai", "anthropic"), you must set the corresponding API keys as environment variables.

*   **For OpenAIJudge:**
    Set the `OPENAI_API_KEY` environment variable:
    ```bash
    export OPENAI_API_KEY="your_openai_api_key_here"
    ```
*   **For AnthropicJudge:**
    Set the `ANTHROPIC_API_KEY` environment variable:
    ```bash
    export ANTHROPIC_API_KEY="your_anthropic_api_key_here"
    ```
If these environment variables are not set, attempting to use the respective judges will result in a warning, and those judges will be skipped during the benchmark run.

### Running the Benchmark CLI

The primary way to evaluate models is through the `run_benchmark` command in the CLI.

**Command Structure:**

```bash
python biaswipe/cli.py [OPTIONS]
```
*(Note: Previous versions might have used `python biaswipe/cli.py run_benchmark [OPTIONS]`. The `run_benchmark` subcommand is now implicit if not using subcommands explicitly).*

**Key Options:**

*   `--prompts <file_path>`: (Required) Path to the JSON file containing prompts.
*   `--annotations <file_path>`: (Required) Path to the JSON file containing human annotations.
*   `--model-responses-dir <directory_path>`: (Required) Path to the directory containing model response JSON files.
*   `--category-weights <file_path>`: (Optional) Path to the JSON file containing category weights for WOSI.
*   `--report-output <file_path>`: (Optional) Path to save the JSON report. Defaults to `./report.json`.
*   `--judges <judge_names>`: (Optional) Comma-separated list of judge names to use for evaluation.
    *   Available names: `mock`, `openai`, `anthropic`.
    *   Example: `--judges openai,mock`
    *   Defaults to `mock` if not provided.
    *   If a judge requires an API key (e.g., `openai`, `anthropic`) and the corresponding environment variable is not set, that judge will be skipped with a warning.

**Example Usage:**

```bash
python biaswipe/cli.py \
    --prompts sample_data/prompts.json \
    --annotations sample_data/annotations.json \
    --model-responses-dir sample_data/model_responses \
    --category-weights sample_data/category_weights.json \
    --judges openai,mock \
    --report-output report.json
```

This command will:
1.  Load prompts, annotations, and model responses.
2.  Use the "openai" and "mock" judges to evaluate the responses.
    *   `OpenAIJudge` will attempt to use the `OPENAI_API_KEY` environment variable.
    *   `MockJudge` provides pre-defined alternating responses.
3.  Calculate metrics: SR (Stereotyping Rate), SSS (Stereotype Severity Score), CSSS (Category-Specific Stereotype Severity), and WOSI (Weighted Overall Stereotype Index) based on the consensus of the selected judges.
4.  Save the results to `report.json`.

### Web-Based Report Viewer

(Content of this section remains the same as before - ensuring `report.json` is in the root for the viewer to pick up)

To start the web viewer, run the `webserver.py` script from the project's root directory:
```bash
python biaswipe_viewer/webserver.py
```
Access the report at `http://127.0.0.1:5000/`. The viewer expects `report.json` in the project root.

## Metrics

(Content of this section remains largely the same - metrics are now based on judge consensus)

The benchmark calculates the following key metrics based on the consensus judgment from the selected AI judges:

*   **SR (Stereotyping Rate):** The proportion of model responses identified as stereotyping.
*   **SSS (Stereotype Severity Score):** The average severity score of responses identified as stereotyping.
*   **CSSS (Category-Specific Stereotype Severity):** The SSS calculated independently for each category of prompts.
*   **WOSI (Weighted Overall Stereotype Index):** An overall stereotype score calculated by taking a weighted average of the CSSS scores.

## Contributing

(Placeholder for future contribution guidelines)

## License

(Placeholder for license information - e.g., MIT, Apache 2.0)

```
