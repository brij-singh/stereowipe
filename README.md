# stereowipe
Data and code for stereowipe, a comprehensive stereotyping evaluation benchmark for LLMs

## Overview

StereoWipe is a benchmark designed to evaluate Large Language Models (LLMs) for stereotyping. It provides datasets, metrics, and tools to assess the extent and nature of stereotypical associations present in model responses.

## Project Structure (Key Components)

- `biaswipe/`: Core Python package for the benchmark.
    - `cli.py`: Command-line interface for running benchmarks.
    - `data_loader.py`: Utilities for loading prompts, annotations, and model responses.
    - `metrics.py`: Implementation of stereotyping metrics (SR, SSS, CSSS, WOSI).
    - `scoring.py`: Main logic for scoring model responses against prompts.
    - `report.py`: Utility for generating JSON reports from benchmark results.
- `biaswipe_viewer/`: Flask web application for viewing reports.
    - `webserver.py`: The Flask web server.
    - `templates/`: HTML templates for the web viewer.
    - `static/`: Static assets (CSS, JS - if any in future).
- `sample_data/`: Directory containing sample JSON files for prompts, annotations, model responses, and category weights.
- `tests/`: Unit tests for the project.
- `requirements.txt`: Python package dependencies.
- `report.json`: Default output file for benchmark reports generated by the CLI.

## Getting Started

### Installation

1.  Clone the repository:
    ```bash
    git clone <repository_url>
    cd stereowipe
    ```
2.  Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### Running the Benchmark CLI

The primary way to evaluate models is through the `run_benchmark` command in the CLI.

**Command Structure:**

```bash
python biaswipe/cli.py run_benchmark [OPTIONS]
```

**Key Options:**

*   `--prompts <file_path>`: (Required) Path to the JSON file containing prompts. Each prompt should include a "text" and a "category".
*   `--annotations <file_path>`: (Required) Path to the JSON file containing human annotations (used for reference, not directly in all current metrics).
*   `--model-responses-dir <directory_path>`: (Required) Path to the directory containing model response JSON files. Each file in this directory is treated as a separate model's responses, with the filename (sans `.json`) as the model name. The JSON file should map prompt IDs to response strings.
*   `--category-weights <file_path>`: (Optional) Path to the JSON file containing category weights for WOSI calculation. This file should be a dictionary mapping category names (strings) to numerical weights.
*   `--report-output <file_path>`: (Optional) Path to save the generated JSON report. Defaults to `./report.json`.

**Example Usage:**

```bash
python biaswipe/cli.py run_benchmark \
    --prompts sample_data/prompts.json \
    --annotations sample_data/annotations.json \
    --model-responses-dir sample_data/ \
    --category-weights sample_data/category_weights.json \
    --report-output report.json
```

This command will:
1.  Load prompts from `sample_data/prompts.json`.
2.  Load annotations from `sample_data/annotations.json`.
3.  Load model responses from all `.json` files within the `sample_data/` directory (e.g., `sample_data/model_responses.json` would be treated as responses from a model named "model_responses").
4.  Load category weights from `sample_data/category_weights.json`.
5.  Calculate metrics: SR (Stereotyping Rate), SSS (Stereotype Severity Score), CSSS (Category-Specific Stereotype Severity), and WOSI (Weighted Overall Stereotype Index).
6.  Save the results to `report.json`.

### Web-Based Report Viewer

The project includes a simple web-based viewer to display the `report.json` generated by the CLI.

**Installation for Viewer:**

The web viewer requires Flask. Ensure you have installed all dependencies from `requirements.txt`:
```bash
pip install -r requirements.txt
```

**Running the Viewer:**

To start the web viewer, run the `webserver.py` script from the project's root directory:
```bash
python biaswipe_viewer/webserver.py
```
Once started, you can access the report by opening your web browser and navigating to `http://127.0.0.1:5000/`.

**Report File Location:**

The web viewer expects the benchmark report file, named `report.json`, to be present in the root directory of the project.
You can generate or update this file by running the BiasWipe CLI tool as shown in the "Running the Benchmark CLI" section. For example:
```bash
python biaswipe/cli.py run_benchmark --prompts sample_data/prompts.json --annotations sample_data/annotations.json --model-responses-dir sample_data/ --report-output report.json
```

## Metrics

The benchmark calculates the following key metrics:

*   **SR (Stereotyping Rate):** The proportion of model responses flagged as stereotyping by the detector.
*   **SSS (Stereotype Severity Score):** The average severity score of responses flagged as stereotyping.
*   **CSSS (Category-Specific Stereotype Severity):** The SSS calculated independently for each category of prompts.
*   **WOSI (Weighted Overall Stereotype Index):** An overall stereotype score calculated by taking a weighted average of the CSSS scores. Category weights can be defined by the user.

## Contributing

(Placeholder for future contribution guidelines)

## License

(Placeholder for license information - e.g., MIT, Apache 2.0)
